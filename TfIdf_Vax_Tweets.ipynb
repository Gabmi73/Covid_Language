{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TfIdf_Vax_Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gabmi73/Covid_Language/blob/master/TfIdf_Vax_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgWUBNA8S_KH"
      },
      "source": [
        "## **Relevant libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMG6bo25KhLX",
        "outputId": "2aece458-794e-4209-9ce6-d77c1fbcd3e2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDPz63I2L4ai"
      },
      "source": [
        "!python -m spacy download it_core_news_sm\n",
        "\n",
        "## restart kernel after dowonload "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-d29MSSPYY8",
        "tags": []
      },
      "source": [
        "import spacy\n",
        "#from collections import Counter\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import random \n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "pd.options.mode.chained_assignment = None\n",
        "pd.set_option('display.max_colwidth', 700)\n",
        "\n",
        "nlp = spacy.load(\"it_core_news_sm\")\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLjQ8gzSfpJP"
      },
      "source": [
        "## **LOADING TWEETS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtGEwD54YK52",
        "tags": []
      },
      "source": [
        "#set my colab directory\n",
        "%cd /content/drive/MyDrive/..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP3o5SC-KZNg",
        "tags": []
      },
      "source": [
        "filenames = [file for file in glob.glob(\"*.csv\")]\n",
        "df_list = [pd.read_csv(file, engine='python', header=0, encoding='utf-8') for file in filenames]\n",
        "\n",
        "# creating and adjusting DataFrame\n",
        "df = pd.concat(df_list, axis=0, ignore_index=False)\n",
        "df.drop_duplicates(subset='status_id', keep='first', inplace=True)\n",
        "df.reset_index(inplace=True)\n",
        "df.rename(columns={'created_at':'date'} , inplace=True)\n",
        "df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "df['week_date'] = df.apply(lambda row: row['date'] - dt.timedelta(days=row['date'].weekday()), axis=1)\n",
        "df['week_date'] = df['week_date'].astype(str)\n",
        "df.drop(columns=['index'], inplace=True)\n",
        "df['date'] = df['date'].astype(str)\n",
        "df['words_nr'] = df['text'].str.split().str.len()\n",
        "df['percentOfWords'] = df['words_nr']/sum(df['words_nr'])*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1eReJo_KZNi"
      },
      "source": [
        "Reducing dataframe and grouping tweets by single day (nr 63 documents total)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3u8E7eCKZNj",
        "tags": []
      },
      "source": [
        "df_rid = df[['date','text','percentOfWords','words_nr']].groupby(['date'], as_index = False).agg({'text': ','.join, 'percentOfWords':sum ,'words_nr':sum})\n",
        "df_rid = df_rid.round({'percentOfWords': 2})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYwurXaiczlv"
      },
      "source": [
        "df_rid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v30f5zdqfYEY"
      },
      "source": [
        "## **PREPROCESSING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K24VRX4Gj4ue"
      },
      "source": [
        "### 1) Get rid of twitter's stuff (links, symbols, @users, html etc) and: punct, digits, uppercases\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2itjRIBBkTCZ"
      },
      "source": [
        "def tweetsClean(text):\n",
        "\n",
        "  text = re.sub(r'(@\\w+)', '', text) # remove users\n",
        "  text = re.sub(r'(#\\w+)', '', text) # remove hashtags\n",
        "  text = re.sub(r'http\\S+', '', text) # remove http links\n",
        "  text = re.sub(r'bit.ly/\\S+', '', text) # rempve bitly links\n",
        "  text = text.strip('[link]') # remove other type links\n",
        "  text = re.sub(r'<.*?>', '', text) # remove html tags\n",
        "  text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '', text) # remove emails\n",
        "  text = re.sub(r'[\\d+]', '', text) # remove mumbers\n",
        "  text = re.sub(r'(&amp)', ' ', text)\n",
        "  text = re.sub(r'(amp)', ' ', text)\n",
        "  text = re.sub(r'\\r\\n', ' ', text)\n",
        "  text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,“”€£$%’'&»«*‘…_]\", ' ', text) # remove special characters and punctuation\n",
        "  text = text.lower() # lowercase\n",
        "  text = text.strip() # get rid of white spaces\n",
        "  text = re.sub(' +', ' ', text) # get rid of extra white spaces\n",
        "\n",
        "  return text\n",
        "\n",
        "df_rid['cleaned_text'] = df_rid.text.apply(tweetsClean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjsnhB75KZNk"
      },
      "source": [
        "### 2) Remove Italian stopwords and other stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV7D248oKZNl"
      },
      "source": [
        "# get rid also of common topics \"covid, vaccine etc.\"\n",
        "def removeStopWords(str):\n",
        "  stopwords = nlp.Defaults.stop_words\n",
        "  stopwords = list(stopwords)\n",
        "  stopwords.extend(['covid', 'è', 'e', 'o', 'l', 'ª', 'd', 'a', 'i','ce', 'c',\n",
        "                    'coronavirus', 'vaccino', 'vaccinare', 'vaccine',\n",
        "                    'vaccinazione', 'aa', 'aaa', 'aaah', 'xché', 'xchè',\n",
        "                    'sò', 'je', 'ke'])\n",
        "  new_str = ' '.join([word for word in str.split() if word not in stopwords]) \n",
        "  return new_str\n",
        "\n",
        "df_rid['cleaned_text'] = df_rid.cleaned_text.apply(removeStopWords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbTGl8YvlM3I"
      },
      "source": [
        "### 3) Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htxBXP_wlaZX",
        "tags": []
      },
      "source": [
        "lemma = []\n",
        "\n",
        "for doc in nlp.pipe(df_rid['cleaned_text'].values):\n",
        "    if doc.is_parsed:\n",
        "        lemma.append([n.lemma_ for n in doc])\n",
        "    else:\n",
        "        lemma.append(None)\n",
        "\n",
        "df_rid['lemma'] = lemma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkTs8x5EKZNm"
      },
      "source": [
        "## **Tf-Idf Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcJEDF5pKZNl"
      },
      "source": [
        "### Create BOW Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpQp3HhhKZNm",
        "tags": []
      },
      "source": [
        "dictionary = Dictionary(df_rid['lemma'].values)\n",
        "dictionary.filter_extremes(no_below=8, no_above=0.5, keep_n=5000)\n",
        "bow_corpus = [dictionary.doc2bow(lemma) for lemma in lemma]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aK_9nxyvgug"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW-WJuzBKZNm"
      },
      "source": [
        "tfidf = TfidfModel(bow_corpus, smartirs='ntc')\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "corpus = [[(dictionary[id], count) for id, count in line] for line in corpus_tfidf]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-3ZaxX-MJ4K"
      },
      "source": [
        "### Extract top 150 most peculiar words for each day of tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1XnkR4nKZNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a643086c-0f77-4afe-e0bc-036cc6f95227"
      },
      "source": [
        "def Sort(tfidf_tuples): \n",
        "    tfidf_tuples.sort(key = lambda x: x[1], reverse=True) \n",
        "    return tfidf_tuples \n",
        "# print(Sort(corpus[1])[:25])\n",
        "\n",
        "most150 = pd.DataFrame([Sort(day)[:150]  for day in corpus]).apply(pd.Series.explode).astype(str).reset_index(drop = True)\n",
        "most150 = most150.iloc[::2].reset_index(drop = True)\n",
        "most150 = most150.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "\n",
        "print(most150)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0     isteria precauzionale collettivo compiere perfido spagna psicosi unificato allarmismo pompare personalmente casalino borsa agitare rezza macron discorrere temporaneo bruciare venosa irrazionale olanda fiatare manica casino comunicativo dominare pre invocare locatelli piú ê archiviare assordare capolavoro pasto ristoro suddito treviso vigliacco allucinare emergere provato belgio bill premettere affannare dividere divisare interruzione man oki parrucchiera pietra placare rigore rincoglionire sciacallo slovenia mancato accertamento stoppare primula conta aspirina scontare dimensione ossigenare risata benedire matematico tromboembolia venuto delirio fallare fermarsi inesistente leggermente m...\n",
            "1     precauzionale panico cooke allarmismo legame emer macron cardiaco psicosi biella esecutivo tromboembolici bus giornalismo statisticamente infarto titolone palù giornalistico temporaneo tranquillizzare lotto credibilità conta cautelativo spagna svezia casino isteria comunicativo cautelare incertezza buonsenso urbano titolo pronunciare accertare collettivo incidere causa certamente aspirina pdc anticoncezionale viagra coincidenza domenico emotivo riparte compromettere causale riunione ingiustificato razionale testata cieco ministro aspera merkel unificato vigilanza sfiga matematico verifica incidenza disinformazione ehrlich residuo rilevato scudo episodio temporale correlare pancia ucciso ...\n",
            "2     iniettarci elemento risarcimento macron metro merkel panico saluto estinguere biella paul omaggio garattini premier assassinare prescindere recuperare discorrere von emotivo precauzionale ehrlich leyen sfiga casino autobus ridare agadala pecora cautelativo angelo sprecare causalità anticoncezionale curiosare fulmine magistratura putin lunare rassicurazione spagna titolo dominare confliggere allarmismo seriamente consumere assordare accertare camera condizionare elettorale bassetti pianeta immortale fidatevi industriare adottare palù clima coronare facciamolo latta der scudo ministro affare sovranisti concorrere ordinato incidere lotto crisanti sconfitto bus distinzione silvio sondaggio a...\n",
            "3     emer iniettarci tromboembolici verdetto cooke elemento superiore riparte foglietto associare illustrativo legame pronunciamento recuperare esecutivo ricominciare precauzionale perso gallina riunione comitato relazione ripartono trombotici medicinale spagna avvertenza prevedibile bergamo anticoncezionale prezioso unificato soddisfazione pm riabilitare pronunciare supposta pregresso cautelativo svezia palù collegamento inutilmente retto verifica pubblicamente causalità rassicurazione casino titolone scontato incrementare incidere norvegese export conclusione dimettersi slittare rischioso panico titolo scommettere ridare ricostruire riconquistare audizione controllo sprecare sbloccare alime...\n",
            "4     gene adenovirus aggiuntare iniettarci spike proteina ricercatore dna ripartono riparte rimettere revocare gesto magrini ripresa whatsapp divieto down locatelli distruggere vespa premier svezia gallina disdire curcio rezza testimonial recuperare instagram associare elemento sponsorizzare macron finlandia pordenone ricominciare causalità illustrativo enrico merkel esito sostegno rassicurazione angela foglietto anglo carro medicinale iamente rivolgersi sopportare boris travolgere ristoro nouns soddisfatto bano css verdetto sin temporaneo salvini dg fabrizio panico riabilitare disinformazione nicola raccogliere norvegese bergamo causale pronunciare autorevole spettacolo lotto fisiologico epi...\n",
            "                                                                                                                                                                                                                                                                                                                                                                 ...                                                                                                                                                                                                                                                                                                                                                             \n",
            "58    sconcertare caserta locatelli intervallare neonato decadere allungare giornalistico mollare prolungare ceo caserma marinare cup raramente azionista profitto stagionale pediatra pacchetto cinquantenne belgio assemblea successone ispezione quarantennio ragazza attento reuters minimamente salvifico neutralizzare bonus gonfiare quartiere francare farmacovigilanza gravidanza tragico attuare cifrare intrugliare spike manager out amen complessivamente norvegese premere impeccabile portato impressione rinviare tesserare industriare indiano aggiuntivo massiccio presentarsi raddoppiare comitato esportazione favorire intellettuale lavaggio replicare turismo adottare dubbioso incrociare pasticciare ...\n",
            "59    creativo posticipare crisanti out derogare caserta collaborare traguardo biglietto obbedire denigratorio ferito virtuoso assaltare metro sessantenne cosenza ribellare quarantennio caffè indiano leggermente tutorial distinzione fegato pancia concertare scienziata sindaco gesto numeroso progettare adolescente cts abbassare settimanale ano paradosso terrorizzato gesù incerto polmonite vaiolo sintomatico doveroso serietà janssen comandare weekend macellare drogare giurare indeciso annuncio tifoso venosa cristo importanza agenda blog flop statunitense sennò ragazza putin schiavo chimico pronunciare mix sbrigare acceleratore apertamente caspita culare esternare eur mò orso salvavita scontato s...\n",
            "60    doc rigare fondo out ferrara vaiolo corto industriare reithera controllato lupo finanziamento manipolazione tosto profumare colposo deludere brivido palare palestinese misterioso omicidio fallare desiderio tasso detenuto addio lotteria torno tranquillità caserta lorusso be incentivare microchip strumento contributo premiare contrattuale spuntare jenner sottoscrivere esaurire disorganizzazione gustare intelligenza formulare italico orario don australia agenda comunità giustiziare prenotarmi scudo eleonora farmacovigilanza basilicata mail mentale sfidare facebook petizione archiviare opel cercasi insabbiare irrilevante maschile orso performance sapientoni sopratutto sovranista spossatezza ...\n",
            "61    corto finanziamento sociopatici toti bocciare formulare fontana instagram xké turista diviso iss ferrara venghino inoculazioni lingua chiamatemi flop nuvolo iscriviti ospitare incentivare out concesso grandissima reithera infettarsi call fiscale fuggire teatro calendario cov don nè presumere fortemente newsletter professionalità scassare esaurito monoclonali esclusivamente riflessione spalla bastone apertamente artificiale becco casuale dinamico fantasia fermato follow lunedi necessariamente off realizzazione sanguigno semaforo soddisfatto tatuaggio vitamina ar martirio spiare mettermi riformare discendere pirla profumare politicante cialtrone stupido gara progettare annientare esito mai...\n",
            "62    cognizione provenienza sardo informarsi vincenzo coetaneo deludere salvezza bicchiere equivalere facilmente pasto denigratorio esternare health off modesto idolo decadere box effettivo rincoglionire good orgoglio vestire volutamente amato papa tendenza austriaco lisciare opposto rene reputare cautela ripercussione biologico mero raffaele min biella implicare claudio italiota mil risalire lorusso esigenza allestire generico invitato razionalità selfie grandissima bergamo ulteriormente combinazione eccezionale organismo sequestrato eff fabbricare puntuale attestare brivido correttamente grato matrimonio offendere cocktail sessantenne annuncio avv for gelare organizzato parallelo quarantenn...\n",
            "Length: 63, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcrhRg_ZMhwl"
      },
      "source": [
        "### Merge to df_rid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i206zR3B5j12"
      },
      "source": [
        "most_150 = pd.DataFrame(most150)\n",
        "most_150.rename(columns = {0: \"top150peculiar_words\"}, inplace = True)\n",
        "df2 = df_rid.join(most_150)\n",
        "df2 = df2[['date', 'top150peculiar_words', 'percentOfWords', 'words_nr']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw6cuT8roNvS"
      },
      "source": [
        "## Wordcloud of some grouped and single days"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voSJoQDXg9-X"
      },
      "source": [
        "days = [\n",
        "        str([most150[1], most150[2], most150[3], most150[4]]),\n",
        "        str([most150[22], most150[23], most150[24], most150[25]]),\n",
        "        str([most150[0]]),\n",
        "        str([most150[42]]),\n",
        "        str([most150[49]]) \n",
        "        ]\n",
        "\n",
        "dates = str([\"16to19-Mar-21\", \"06to09-Apr-21\", \"15-Mar-21\", \"26-Apr-21\", \"03-May-21\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I58Fy7JBQ_cJ"
      },
      "source": [
        "%cd /content/drive/MyDrive/...",
        "df2.to_csv(\"most150_tfidf.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A193hoDNRhZ"
      },
      "source": [
        "%cd /content/drive/MyDrive/.../images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9p9K-lA2wjL"
      },
      "source": [
        "for day in days:\n",
        "  wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(day)\n",
        "  plt.figure(figsize=(15,8))\n",
        "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "  plt.axis('off')\n",
        "  plt.savefig(\"word_{}.png\".format(str(days.index)))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
